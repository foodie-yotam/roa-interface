<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>ROA Voice</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/jpeg" href="logo_foodweb.jpg">
    
    <!-- PWA Configuration -->
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#091A05">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="apple-mobile-web-app-title" content="ROA Voice">
    <link rel="apple-touch-icon" href="logo_foodweb.jpg">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            -webkit-tap-highlight-color: transparent;
        }

        body {
            background-color: #091A05;
            width: 100vw;
            height: 100vh;
            overflow: hidden;
            display: flex;
            justify-content: center;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        }

        .voice-button {
            position: relative;
            width: min(80vw, 400px);
            aspect-ratio: 1;
            border-radius: 50%;
            overflow: hidden;
            cursor: pointer;
            user-select: none;
            -webkit-user-select: none;
            touch-action: none;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.4);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .voice-button:active {
            transform: scale(0.95);
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.6);
        }

        .voice-button video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .status-text {
            position: absolute;
            bottom: 10%;
            left: 50%;
            transform: translateX(-50%);
            color: #fff;
            font-size: clamp(14px, 4vw, 18px);
            font-weight: 500;
            text-align: center;
            text-shadow: 0 2px 8px rgba(0, 0, 0, 0.8);
            padding: 8px 16px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 20px;
            backdrop-filter: blur(10px);
        }

        .text-display {
            position: absolute;
            top: 8%;
            left: 50%;
            transform: translateX(-50%);
            width: min(90vw, 600px);
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
            border-radius: 16px;
            padding: 20px;
            color: #fff;
            font-size: clamp(14px, 3.8vw, 18px);
            line-height: 1.6;
            text-align: center;
            min-height: 60px;
            display: none;
            transition: all 0.3s ease;
        }

        .text-display.visible {
            display: block;
        }

        /* Pulsing animation for recording state */
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }

        .recording {
            animation: pulse 1.5s ease-in-out infinite;
        }
    </style>
</head>
<body>
    <div class="text-display" id="textDisplay"></div>

    <div class="voice-button" id="voiceBtn">
        <video id="logoVideo" muted playsinline>
            <source src="LOGO-ANIMATE.mp4" type="video/mp4">
        </video>
    </div>

    <div class="status-text" id="statusText">Press and hold to speak</div>

    <script>
        const voiceBtn = document.getElementById('voiceBtn');
        const logoVideo = document.getElementById('logoVideo');
        const statusText = document.getElementById('statusText');
        const textDisplay = document.getElementById('textDisplay');

        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;
        let currentAudio = null;
        let currentTimeout = null;
        let requestTimestamp = 0;
        let recognition = null;
        let transcriptText = '';

        // Initialize Web Speech API (if available)
        const USE_WEB_SPEECH = true; // Set to false to use Whisper API
        
        if (USE_WEB_SPEECH && ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = false;
            recognition.lang = 'en-US'; // Change to 'he-IL' for Hebrew, 'ar-SA' for Arabic, etc.
            
            recognition.onresult = (event) => {
                transcriptText = event.results[0][0].transcript;
                console.log('üéß [Web Speech] Transcript:', transcriptText);
            };
            
            recognition.onerror = (event) => {
                console.error('‚ùå [Web Speech] Error:', event.error);
            };
            
            console.log('‚úÖ [Frontend] Web Speech API initialized');
        } else if (USE_WEB_SPEECH) {
            console.warn('‚ö†Ô∏è [Frontend] Web Speech API not available, falling back to Whisper');
        }

        // Show text in display box
        function showText(text) {
            textDisplay.textContent = text;
            textDisplay.classList.add('visible');
        }

        // Hide text display
        function hideText() {
            textDisplay.classList.remove('visible');
        }

        // Stop everything - audio, timeouts, etc
        function stopEverything() {
            console.log('üõë [Frontend] STOPPING EVERYTHING');
            
            // Stop any playing audio
            if (currentAudio) {
                currentAudio.pause();
                currentAudio.currentTime = 0;
                currentAudio = null;
            }
            
            // Clear any pending timeouts
            if (currentTimeout) {
                clearTimeout(currentTimeout);
                currentTimeout = null;
            }
            
            // Hide text display
            hideText();
        }

        // Video control
        function startVideo() {
            logoVideo.loop = true;
            logoVideo.play().catch(err => console.log('Video play failed:', err));
        }

        function stopVideo() {
            logoVideo.loop = false;
            logoVideo.pause();
        }

        // Handle press start (mouse/touch)
        function handlePressStart(e) {
            e.preventDefault();
            if (isRecording) return;

            console.log('üîò [Frontend] Press started');
            
            // STOP EVERYTHING IMMEDIATELY
            stopEverything();
            
            // Generate new timestamp for this request
            requestTimestamp = Date.now();
            
            startVideo();
            voiceBtn.classList.add('recording');
            startRecording();
        }

        // Handle press end (mouse/touch)
        function handlePressEnd(e) {
            e.preventDefault();
            if (!isRecording) return;

            console.log('üîò [Frontend] Press ended');
            stopVideo();
            voiceBtn.classList.remove('recording');
            stopRecording();
        }

        // Recording functions
        async function startRecording() {
            try {
                console.log('‚ñ∂Ô∏è [Frontend] Starting recording...');
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,    // Lower sample rate for smaller file
                        channelCount: 1,       // Mono audio
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                console.log('‚úÖ [Frontend] Microphone access granted');

                // Try opus codec first (smallest files), fall back to webm, then default
                let options = { audioBitsPerSecond: 16000 };
                
                if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
                    options.mimeType = 'audio/webm;codecs=opus';
                    console.log('üéµ [Frontend] Using Opus codec (high compression)');
                } else if (MediaRecorder.isTypeSupported('audio/webm')) {
                    options.mimeType = 'audio/webm';
                    console.log('üéµ [Frontend] Using WebM format');
                } else {
                    console.log('üéµ [Frontend] Using default codec');
                }

                mediaRecorder = new MediaRecorder(stream, options);
                audioChunks = [];
                isRecording = true;

                mediaRecorder.ondataavailable = (event) => {
                    console.log('üìº [Frontend] Audio chunk received:', event.data.size, 'bytes');
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    console.log('‚èπÔ∏è [Frontend] Recording stopped');
                    console.log('üì¶ [Frontend] Total chunks:', audioChunks.length);
                    
                    // Use the same mime type as recording
                    const mimeType = mediaRecorder.mimeType || 'audio/webm';
                    const audioBlob = new Blob(audioChunks, { type: mimeType });
                    console.log('üéµ [Frontend] Audio blob created:', audioBlob.size, 'bytes', `(${mimeType})`);
                    console.log('üìä [Frontend] Compression ratio:', audioBlob.size < 200000 ? '‚úÖ Good' : '‚ö†Ô∏è Large');
                    await processAudio(audioBlob);
                };

                mediaRecorder.start();
                console.log('üéôÔ∏è [Frontend] MediaRecorder started');
                statusText.textContent = 'üé§ Recording...';

            } catch (error) {
                console.error('‚ùå [Frontend] Microphone access error:', error);
                statusText.textContent = '‚ùå Microphone access denied';
                isRecording = false;
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                console.log('‚èπÔ∏è [Frontend] Stopping recording...');
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => {
                    console.log('üîá [Frontend] Stopping track:', track.kind, track.label);
                    track.stop();
                });
                statusText.textContent = '‚è≥ Processing...';
                isRecording = false;
            }
        }

        // Process audio
        async function processAudio(audioBlob) {
            // Capture timestamp when starting this request
            const myTimestamp = requestTimestamp;
            
            console.log('üéôÔ∏è [Frontend] Starting audio processing');
            console.log('üìä [Frontend] Audio blob size:', audioBlob.size, 'bytes');
            console.log('‚è±Ô∏è [Frontend] Request timestamp:', myTimestamp);

            const formData = new FormData();
            formData.append('audio', audioBlob);
            formData.append('user_id', 'web-user');

            try {
                console.log('üåê [Frontend] Sending request to backend...');
                const startTime = Date.now();

                const res = await fetch('https://roa-server-production.up.railway.app/process_voice', {
                    method: 'POST',
                    body: formData
                });

                const responseTime = Date.now() - startTime;
                console.log(`‚è±Ô∏è [Frontend] Response received in ${responseTime}ms`);
                console.log('üì° [Frontend] Response status:', res.status, res.statusText);

                const data = await res.json();
                console.log('üì¶ [Frontend] Response data:', data);

                // Check if this response is still relevant (not superseded by new recording)
                if (myTimestamp !== requestTimestamp) {
                    console.log('üö´ [Frontend] Ignoring stale response (new recording started)');
                    return;
                }

                if (data.error) {
                    console.error('‚ùå [Frontend] Error from backend:', data.error);
                    statusText.textContent = '‚ùå Error: ' + data.error;
                    return;
                }

                console.log('‚úÖ [Frontend] Transcript:', data.transcript);
                console.log('üí¨ [Frontend] Agent response:', data.response);

                // Show user's transcript first
                showText(data.transcript);
                
                // Wait a moment, then show AI response
                currentTimeout = setTimeout(() => {
                    // Check again if still relevant
                    if (myTimestamp !== requestTimestamp) {
                        console.log('üö´ [Frontend] Ignoring stale response in timeout');
                        return;
                    }
                    
                    showText(data.response);
                    statusText.textContent = '‚úÖ Playing response...';

                    // Play audio response
                    if (data.audio_url) {
                        console.log('üîä [Frontend] Playing audio:', data.audio_url);
                        currentAudio = new Audio(data.audio_url);
                        currentAudio.play();
                        currentAudio.onended = () => {
                            if (myTimestamp === requestTimestamp) {
                                statusText.textContent = 'Press and hold to speak';
                                currentAudio = null;
                            }
                        };
                    } else {
                        statusText.textContent = 'Press and hold to speak';
                    }
                }, 1000);

            } catch (error) {
                console.error('üí• [Frontend] Exception caught:', error);
                console.error('üìç [Frontend] Error stack:', error.stack);
                statusText.textContent = '‚ùå Error: ' + error.message;
            }
        }

        // Event listeners for desktop (mouse)
        voiceBtn.addEventListener('mousedown', handlePressStart);
        voiceBtn.addEventListener('mouseup', handlePressEnd);
        voiceBtn.addEventListener('mouseleave', handlePressEnd);

        // Event listeners for mobile (touch)
        voiceBtn.addEventListener('touchstart', handlePressStart);
        voiceBtn.addEventListener('touchend', handlePressEnd);
        voiceBtn.addEventListener('touchcancel', handlePressEnd);

        // Prevent context menu on long press
        voiceBtn.addEventListener('contextmenu', e => e.preventDefault());

        // Register service worker for PWA
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.register('/sw.js')
                .then(registration => console.log('‚úÖ [Frontend] Service Worker registered'))
                .catch(error => console.log('‚ùå [Frontend] Service Worker registration failed:', error));
        }

        console.log('‚úÖ [Frontend] Voice interface initialized');
    </script>
</body>
</html>
